import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin
from urllib.parse import quote_plus


session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Connection": "keep-alive"
})
def get_month_link(page_url):
    page_response = session.get(page_url)

    soup = BeautifulSoup(page_response.text, 'html.parser')

    all_links = soup.find_all('a')
    month_links=[]
    for i in all_links:
        month_links.append(i.get('href'))

    month_links = month_links[5:]

    linked_page_url=[]

    for link in month_links:
        linked_page_url.append(urljoin(page_url, link))

    return linked_page_url
def scrape_documents_from_page(page_url, output_file, terminal_output_file, urls_file):
    try:
        # Send a GET request with headers
        page_response = session.get(page_url)
        print(f"[{page_response.status_code}] {page_url}")

        if page_response.status_code == 200:
            page_soup = BeautifulSoup(page_response.text, 'html.parser')
            links = page_soup.find_all('a', href=True)

            for link in links:
                linked_page_url = urljoin(page_url, link['href'])

                if 'doctypes' in linked_page_url:
                    document_response = session.get(linked_page_url)

                    if document_response.status_code == 200:
                        document_soup = BeautifulSoup(document_response.text, 'html.parser')
                        document_text_div = document_soup.find('div', class_='judgments')

                        if document_text_div:
                            document_text = document_text_div.get_text(strip=True)

                            with open(urls_file, 'a', newline='', encoding='utf-8') as f:
                                writer = csv.writer(f)
                                writer.writerow([linked_page_url])

                            with open(output_file, 'a', newline='', encoding='utf-8') as f:
                                writer = csv.writer(f)
                                writer.writerow([document_text])
                        else:
                            print(f"‚ö†Ô∏è 'judgments' div not found in: {linked_page_url}")
                    else:
                        print(f"‚ùå Failed to get document page: {linked_page_url} ({document_response.status_code})")

            next_link = page_soup.find('a', text='Next')
            if next_link:
                next_page_url = urljoin(page_url, next_link['href'])
                scrape_documents_from_page(next_page_url, output_file, terminal_output_file, urls_file)

        else:
            print(f"‚ùå Failed to get main page: {page_url} ({page_response.status_code})")
    except Exception as e:
        print(f"üî• Exception while scraping {page_url}: {str(e)}")


# -----------------------------------------------------------------------------------------------
# change url here only
links=get_month_link("https://indiankanoon.org/browse/supremecourt/2024/")
# -----------------------------------------------------------------------------------------------

for main_page_url in links:
    print(main_page_url)
    output_file = 'SC_Output_2024.csv'
    terminal_output_file = 'terminal_output.csv'
    urls_file = 'SC_Url_2024.csv'

    scrape_documents_from_page(main_page_url, output_file, terminal_output_file, urls_file)
